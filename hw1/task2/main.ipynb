{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text, label = line.strip().split('\\t')\n",
    "            tokens = list(jieba.cut(text))\n",
    "            texts.append(tokens)\n",
    "            labels.append(int(label))\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_data('train.txt')\n",
    "dev_texts, dev_labels = load_data('dev.txt')\n",
    "test_texts, test_labels = load_data('test.txt')\n",
    "\n",
    "# Build vocabulary\n",
    "from collections import Counter\n",
    "all_tokens = [token for text in train_texts for token in text]\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(Counter(all_tokens).most_common())}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "# Encoding and padding\n",
    "def encode_texts(texts):\n",
    "    encoded_texts = []\n",
    "    for tokens in texts:\n",
    "        encoded = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "        if len(encoded) < MAX_LEN:\n",
    "            encoded += [vocab['<PAD>']] * (MAX_LEN - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:MAX_LEN]\n",
    "        encoded_texts.append(encoded)\n",
    "    return np.array(encoded_texts)\n",
    "\n",
    "train_inputs = encode_texts(train_texts)\n",
    "dev_inputs = encode_texts(dev_texts)\n",
    "test_inputs = encode_texts(test_texts)\n",
    "train_labels = np.array(train_labels)\n",
    "dev_labels = np.array(dev_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Create Datasets and Dataloaders\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = torch.LongTensor(inputs)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TextDataset(train_inputs, train_labels)\n",
    "dev_dataset = TextDataset(dev_inputs, dev_labels)\n",
    "test_dataset = TextDataset(test_inputs, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_classes=2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, 100, (k, embed_dim)) for k in [3, 4, 5]\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(300, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [B, L, D]\n",
    "        x = x.unsqueeze(1)     # [B, 1, L, D]\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(B, Co, Lk), ...]\n",
    "        x = [F.max_pool1d(item, item.size(2)).squeeze(2) for item in x]  # [(B, Co), ...]\n",
    "        x = torch.cat(x, 1)    # [B, Co * len(Ks)]\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Epoch 1, Dev Loss: 0.8798480965197086\n",
      "Epoch 2, Dev Loss: 0.7621156759560108\n",
      "Epoch 3, Dev Loss: 0.6583794206380844\n",
      "Epoch 4, Dev Loss: 0.6079136319458485\n",
      "Epoch 5, Dev Loss: 0.6292803473770618\n",
      "Epoch 6, Dev Loss: 0.6079052556306124\n",
      "Epoch 7, Dev Loss: 0.6116425096988678\n",
      "Epoch 8, Dev Loss: 0.6058465857058764\n",
      "Epoch 9, Dev Loss: 0.6539240535348654\n",
      "Epoch 10, Dev Loss: 0.7168174311518669\n",
      "Epoch 11, Dev Loss: 0.6745563875883818\n",
      "Epoch 12, Dev Loss: 0.7070010732859373\n",
      "Epoch 13, Dev Loss: 0.6963232941925526\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of classes\n",
    "all_labels = train_labels.tolist() + dev_labels.tolist() + test_labels.tolist()\n",
    "num_classes = len(set(all_labels))\n",
    "# print(f'Number of classes: {num_classes}')  # Should print 4\n",
    "\n",
    "# Update the model initialization\n",
    "VOCAB_SIZE = len(vocab)\n",
    "model = TextCNN(VOCAB_SIZE, num_classes=num_classes)\n",
    "\n",
    "# Proceed with the rest of your code\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = 5\n",
    "best_dev_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            dev_loss += loss.item()\n",
    "    dev_loss /= len(dev_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Dev Loss: {dev_loss}')\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if dev_loss < best_dev_loss:\n",
    "        best_dev_loss = dev_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.40%\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_accuracy = correct / total * 100\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Config of CNN\n",
    "\n",
    "The config of CNN is as follows:\n",
    "\n",
    "```\n",
    "embed_dim = 128\n",
    "num_classes = 4\n",
    "learning_rate = 1e-3\n",
    "early_stopping_patience = 5\n",
    "max_len = 100\n",
    "batch_size = 64\n",
    "max_epochs = 20\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "The test accuracy is 80.40%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hantao_safe_rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[2024-11-15 11:52:32,876] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/15/2024 11:52:33 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmpmgmrewj_/test.c -o /tmp/tmpmgmrewj_/test.o
11/15/2024 11:52:33 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmpmgmrewj_/test.o -laio -o /tmp/tmpmgmrewj_/a.out
11/15/2024 11:52:33 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmpw34_8h_o/test.c -o /tmp/tmpw34_8h_o/test.o
11/15/2024 11:52:34 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmpw34_8h_o/test.o -L/data/align-anything/miniconda3/envs/hantao_cham -L/data/align-anything/miniconda3/envs/hantao_cham/lib64 -lcufile -o /tmp/tmpw34_8h_o/a.out
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                                                                                                                                                                     | 0/270 [00:00<?, ?it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                   | 54/270 [00:42<01:35,  2.26it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.058, 'grad_norm': 2.65126895904541, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.19}
{'loss': 0.9346, 'grad_norm': 1.7624486684799194, 'learning_rate': 1.953125e-05, 'epoch': 0.37}
{'loss': 0.8885, 'grad_norm': 2.5479655265808105, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.56}
{'loss': 0.8409, 'grad_norm': 3.544285535812378, 'learning_rate': 1.7968750000000003e-05, 'epoch': 0.74}
{'loss': 0.776, 'grad_norm': 6.186740875244141, 'learning_rate': 1.71875e-05, 'epoch': 0.93}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.5704428553581238, 'eval_accuracy': 0.7741071428571429, 'eval_micro_f1': 0.7741071428571429, 'eval_macro_f1': 0.5370829426200844, 'eval_runtime': 9.3261, 'eval_samples_per_second': 120.092, 'eval_steps_per_second': 1.93, 'epoch': 1.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                         | 108/270 [01:20<01:11,  2.28it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.634, 'grad_norm': 7.806676864624023, 'learning_rate': 1.6406250000000002e-05, 'epoch': 1.11}
{'loss': 0.6241, 'grad_norm': 8.481544494628906, 'learning_rate': 1.5625e-05, 'epoch': 1.3}
{'loss': 0.5887, 'grad_norm': 9.746624946594238, 'learning_rate': 1.4843750000000002e-05, 'epoch': 1.48}
{'loss': 0.5716, 'grad_norm': 19.314250946044922, 'learning_rate': 1.4062500000000001e-05, 'epoch': 1.67}
{'loss': 0.5286, 'grad_norm': 10.059833526611328, 'learning_rate': 1.3281250000000001e-05, 'epoch': 1.85}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.4552497863769531, 'eval_accuracy': 0.8267857142857142, 'eval_micro_f1': 0.8267857142857142, 'eval_macro_f1': 0.7173648817213851, 'eval_runtime': 10.9914, 'eval_samples_per_second': 101.898, 'eval_steps_per_second': 1.638, 'epoch': 2.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 162/270 [02:00<00:45,  2.37it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.588, 'grad_norm': 9.653031349182129, 'learning_rate': 1.25e-05, 'epoch': 2.04}
{'loss': 0.4764, 'grad_norm': 6.455774307250977, 'learning_rate': 1.171875e-05, 'epoch': 2.22}
{'loss': 0.4663, 'grad_norm': 7.781613349914551, 'learning_rate': 1.0937500000000002e-05, 'epoch': 2.41}
{'loss': 0.4589, 'grad_norm': 8.525188446044922, 'learning_rate': 1.0156250000000001e-05, 'epoch': 2.59}
{'loss': 0.4279, 'grad_norm': 8.001181602478027, 'learning_rate': 9.375000000000001e-06, 'epoch': 2.78}
{'loss': 0.5035, 'grad_norm': 9.750391006469727, 'learning_rate': 8.59375e-06, 'epoch': 2.96}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.44075194001197815, 'eval_accuracy': 0.8258928571428571, 'eval_micro_f1': 0.8258928571428571, 'eval_macro_f1': 0.7113137036209504, 'eval_runtime': 9.9362, 'eval_samples_per_second': 112.72, 'eval_steps_per_second': 1.812, 'epoch': 3.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 216/270 [02:37<00:24,  2.18it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.4464, 'grad_norm': 10.294450759887695, 'learning_rate': 7.8125e-06, 'epoch': 3.15}
{'loss': 0.3718, 'grad_norm': 10.342204093933105, 'learning_rate': 7.031250000000001e-06, 'epoch': 3.33}
{'loss': 0.3987, 'grad_norm': 12.210417747497559, 'learning_rate': 6.25e-06, 'epoch': 3.52}
{'loss': 0.3861, 'grad_norm': 11.874823570251465, 'learning_rate': 5.468750000000001e-06, 'epoch': 3.7}
{'loss': 0.3467, 'grad_norm': 8.972129821777344, 'learning_rate': 4.6875000000000004e-06, 'epoch': 3.89}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.4114564061164856, 'eval_accuracy': 0.8517857142857143, 'eval_micro_f1': 0.8517857142857143, 'eval_macro_f1': 0.7667536300629827, 'eval_runtime': 8.485, 'eval_samples_per_second': 131.997, 'eval_steps_per_second': 2.121, 'epoch': 4.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [03:05<00:00,  2.06it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.3306, 'grad_norm': 7.991728782653809, 'learning_rate': 3.90625e-06, 'epoch': 4.07}
{'loss': 0.3571, 'grad_norm': 7.59161376953125, 'learning_rate': 3.125e-06, 'epoch': 4.26}
{'loss': 0.2967, 'grad_norm': 8.140263557434082, 'learning_rate': 2.3437500000000002e-06, 'epoch': 4.44}
{'loss': 0.3044, 'grad_norm': 9.699822425842285, 'learning_rate': 1.5625e-06, 'epoch': 4.63}
{'loss': 0.286, 'grad_norm': 9.974772453308105, 'learning_rate': 7.8125e-07, 'epoch': 4.81}
{'loss': 0.3512, 'grad_norm': 7.351279258728027, 'learning_rate': 0.0, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [03:22<00:00,  1.33it/s]
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.                                                                                  
{'eval_loss': 0.45773032307624817, 'eval_accuracy': 0.8464285714285714, 'eval_micro_f1': 0.8464285714285714, 'eval_macro_f1': 0.7442050117353881, 'eval_runtime': 10.197, 'eval_samples_per_second': 109.836, 'eval_steps_per_second': 1.765, 'epoch': 5.0}
{'train_runtime': 203.0413, 'train_samples_per_second': 85.007, 'train_steps_per_second': 1.33, 'train_loss': 0.5274726373177988, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:10<00:00,  1.74it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8518
  eval_loss               =     0.4115
  eval_macro_f1           =     0.7668
  eval_micro_f1           =     0.8518
  eval_runtime            = 0:00:10.43
  eval_samples_per_second =    107.318
  eval_steps_per_second   =      1.725

[2024-11-16 19:12:26,939] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/16/2024 19:12:27 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmp6vzdxdtw/test.c -o /tmp/tmp6vzdxdtw/test.o
11/16/2024 19:12:27 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmp6vzdxdtw/test.o -laio -o /tmp/tmp6vzdxdtw/a.out
11/16/2024 19:12:28 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmpu4pxglrg/test.c -o /tmp/tmpu4pxglrg/test.o
11/16/2024 19:12:28 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmpu4pxglrg/test.o -L/data/align-anything/miniconda3/envs/hantao_cham -L/data/align-anything/miniconda3/envs/hantao_cham/lib64 -lcufile -o /tmp/tmpu4pxglrg/a.out
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                                 | 0/135 [00:00<?, ?it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██████████████████████████████▍                                                                                                                         | 27/135 [00:24<00:37,  2.89it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.4009, 'grad_norm': 1.2005469799041748, 'learning_rate': 0.00048828125, 'epoch': 0.37}
{'loss': 1.3765, 'grad_norm': 0.9508941173553467, 'learning_rate': 0.00044921875, 'epoch': 0.74}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                              
{'eval_loss': 1.3594101667404175, 'eval_accuracy': 0.5486842105263158, 'eval_micro_f1': 0.5486842105263158, 'eval_macro_f1': 0.5295427320571326, 'eval_runtime': 8.6474, 'eval_samples_per_second': 87.888, 'eval_steps_per_second': 0.347, 'epoch': 1.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████████████████████████████████████████████████████████████▊                                                                                           | 54/135 [00:45<00:29,  2.78it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.3651, 'grad_norm': 0.9062238931655884, 'learning_rate': 0.00041015625, 'epoch': 1.11}
{'loss': 1.3466, 'grad_norm': 1.060077428817749, 'learning_rate': 0.00037109375, 'epoch': 1.48}
{'loss': 1.3366, 'grad_norm': 1.1453139781951904, 'learning_rate': 0.00033203125, 'epoch': 1.85}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                              
{'eval_loss': 1.3280742168426514, 'eval_accuracy': 0.8118421052631579, 'eval_micro_f1': 0.8118421052631579, 'eval_macro_f1': 0.8057197504238003, 'eval_runtime': 8.3121, 'eval_samples_per_second': 91.433, 'eval_steps_per_second': 0.361, 'epoch': 2.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                            | 81/135 [01:07<00:23,  2.27it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.3229, 'grad_norm': 0.4634450674057007, 'learning_rate': 0.00029296875, 'epoch': 2.22}
{'loss': 1.3185, 'grad_norm': 1.0383052825927734, 'learning_rate': 0.00025390625, 'epoch': 2.59}
{'loss': 1.3078, 'grad_norm': 1.1215674877166748, 'learning_rate': 0.00021484375, 'epoch': 2.96}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                              
{'eval_loss': 1.3071095943450928, 'eval_accuracy': 0.8157894736842105, 'eval_micro_f1': 0.8157894736842105, 'eval_macro_f1': 0.80937955060514, 'eval_runtime': 8.3896, 'eval_samples_per_second': 90.588, 'eval_steps_per_second': 0.358, 'epoch': 3.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 108/135 [01:36<00:11,  2.38it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.3056, 'grad_norm': 0.7781100869178772, 'learning_rate': 0.00017578125, 'epoch': 3.33}
{'loss': 1.2996, 'grad_norm': 0.5816505551338196, 'learning_rate': 0.00013671875, 'epoch': 3.7}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                              
{'eval_loss': 1.2955965995788574, 'eval_accuracy': 0.8355263157894737, 'eval_micro_f1': 0.8355263157894737, 'eval_macro_f1': 0.8300503486558399, 'eval_runtime': 15.055, 'eval_samples_per_second': 50.481, 'eval_steps_per_second': 0.199, 'epoch': 4.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [01:51<00:00,  2.08it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.2935, 'grad_norm': 0.7821047902107239, 'learning_rate': 9.765625e-05, 'epoch': 4.07}
{'loss': 1.293, 'grad_norm': 1.2752842903137207, 'learning_rate': 5.859375e-05, 'epoch': 4.44}
{'loss': 1.2919, 'grad_norm': 0.8994811773300171, 'learning_rate': 1.953125e-05, 'epoch': 4.81}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [02:04<00:00,  1.08it/s]
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'eval_loss': 1.2914631366729736, 'eval_accuracy': 0.8355263157894737, 'eval_micro_f1': 0.8355263157894737, 'eval_macro_f1': 0.8300195312556693, 'eval_runtime': 8.7315, 'eval_samples_per_second': 87.041, 'eval_steps_per_second': 0.344, 'epoch': 5.0}
{'train_runtime': 125.0315, 'train_samples_per_second': 273.531, 'train_steps_per_second': 1.08, 'train_loss': 1.3262595317981862, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.57s/it]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8355
  eval_loss               =     1.2956
  eval_macro_f1           =     0.8301
  eval_micro_f1           =     0.8355
  eval_runtime            = 0:00:07.89
  eval_samples_per_second =     96.296
  eval_steps_per_second   =       0.38

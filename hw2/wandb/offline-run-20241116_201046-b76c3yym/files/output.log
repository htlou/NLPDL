[2024-11-16 20:10:48,621] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
{'loss': 1.405, 'grad_norm': 1.6486403942108154, 'learning_rate': 0.00035714285714285714, 'epoch': 0.19}
{'loss': 1.3849, 'grad_norm': 0.6634284257888794, 'learning_rate': 0.00048828125, 'epoch': 0.37}
{'loss': 1.3711, 'grad_norm': 1.0993247032165527, 'learning_rate': 0.00046875, 'epoch': 0.56}
{'loss': 1.3573, 'grad_norm': 0.9510071277618408, 'learning_rate': 0.00044921875, 'epoch': 0.74}
{'loss': 1.3419, 'grad_norm': 1.0517877340316772, 'learning_rate': 0.0004296875, 'epoch': 0.93}
{'eval_loss': 1.3328526020050049, 'eval_accuracy': 0.7644736842105263, 'eval_micro_f1': 0.7644736842105263, 'eval_macro_f1': 0.7621072021164097, 'eval_runtime': 8.7512, 'eval_samples_per_second': 86.846, 'eval_steps_per_second': 0.686, 'epoch': 1.0}
{'loss': 1.3276, 'grad_norm': 1.1482274532318115, 'learning_rate': 0.00041015625, 'epoch': 1.11}
{'loss': 1.3202, 'grad_norm': 0.780208945274353, 'learning_rate': 0.000390625, 'epoch': 1.3}
{'loss': 1.3122, 'grad_norm': 1.1041244268417358, 'learning_rate': 0.00037109375, 'epoch': 1.48}
{'loss': 1.2995, 'grad_norm': 0.7384266257286072, 'learning_rate': 0.0003515625, 'epoch': 1.67}
{'loss': 1.2918, 'grad_norm': 1.6065479516983032, 'learning_rate': 0.00033203125, 'epoch': 1.85}
{'eval_loss': 1.28470778465271, 'eval_accuracy': 0.8421052631578947, 'eval_micro_f1': 0.8421052631578947, 'eval_macro_f1': 0.8382390744687285, 'eval_runtime': 9.0258, 'eval_samples_per_second': 84.203, 'eval_steps_per_second': 0.665, 'epoch': 2.0}
{'loss': 1.2806, 'grad_norm': 1.6342782974243164, 'learning_rate': 0.0003125, 'epoch': 2.04}
{'loss': 1.273, 'grad_norm': 1.4407017230987549, 'learning_rate': 0.00029296875, 'epoch': 2.22}
{'loss': 1.2708, 'grad_norm': 0.8294168710708618, 'learning_rate': 0.0002734375, 'epoch': 2.41}
{'loss': 1.2657, 'grad_norm': 0.6034096479415894, 'learning_rate': 0.00025390625, 'epoch': 2.59}
{'loss': 1.2589, 'grad_norm': 0.49172067642211914, 'learning_rate': 0.000234375, 'epoch': 2.78}
{'loss': 1.2499, 'grad_norm': 0.9116628170013428, 'learning_rate': 0.00021484375, 'epoch': 2.96}
{'eval_loss': 1.2521986961364746, 'eval_accuracy': 0.8381578947368421, 'eval_micro_f1': 0.8381578947368421, 'eval_macro_f1': 0.8322518132416884, 'eval_runtime': 10.0507, 'eval_samples_per_second': 75.617, 'eval_steps_per_second': 0.597, 'epoch': 3.0}
{'loss': 1.2476, 'grad_norm': 0.499140202999115, 'learning_rate': 0.0001953125, 'epoch': 3.15}
11/16/2024 20:10:48 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmp2mvxzx__/test.c -o /tmp/tmp2mvxzx__/test.o
11/16/2024 20:10:48 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmp2mvxzx__/test.o -laio -o /tmp/tmp2mvxzx__/a.out
11/16/2024 20:10:49 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmpn2vwntr_/test.c -o /tmp/tmpn2vwntr_/test.o
11/16/2024 20:10:49 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmpn2vwntr_/test.o -L/data/align-anything/miniconda3/envs/hantao_cham -L/data/align-anything/miniconda3/envs/hantao_cham/lib64 -lcufile -o /tmp/tmpn2vwntr_/a.out
[34m[1mwandb[39m[22m: [33mWARNING[39m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
[34m[1mwandb[39m[22m: [33mWARNING[39m URL not available in offline run
  0%|                                                                                                                                                                 | 0/270 [00:00<?, ?it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██████████████████████████████▍                                                                                                                         | 54/270 [00:28<00:56,  3.81it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████████████████████████████████████████████████████████████▍                                                                                          | 108/270 [00:53<00:46,  3.45it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████████████████████████████████████████████████████████████████████████████████████████▌                                                            | 162/270 [01:24<00:28,  3.74it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 210/270 [01:40<00:20,  2.91it/s]
{'loss': 1.244, 'grad_norm': 0.7364282011985779, 'learning_rate': 0.00017578125, 'epoch': 3.33}
{'loss': 1.2472, 'grad_norm': 1.317870020866394, 'learning_rate': 0.00015625, 'epoch': 3.52}
{'loss': 1.2388, 'grad_norm': 1.3275233507156372, 'learning_rate': 0.00013671875, 'epoch': 3.7}
{'loss': 1.237, 'grad_norm': 0.8024548292160034, 'learning_rate': 0.0001171875, 'epoch': 3.89}
{'eval_loss': 1.2337368726730347, 'eval_accuracy': 0.85, 'eval_micro_f1': 0.85, 'eval_macro_f1': 0.8453038550500487, 'eval_runtime': 10.5168, 'eval_samples_per_second': 72.265, 'eval_steps_per_second': 0.571, 'epoch': 4.0}
{'loss': 1.2254, 'grad_norm': 0.32115358114242554, 'learning_rate': 9.765625e-05, 'epoch': 4.07}
{'loss': 1.2272, 'grad_norm': 1.1419235467910767, 'learning_rate': 7.8125e-05, 'epoch': 4.26}
{'loss': 1.2258, 'grad_norm': 1.3509327173233032, 'learning_rate': 5.859375e-05, 'epoch': 4.44}
{'loss': 1.2171, 'grad_norm': 1.2423124313354492, 'learning_rate': 3.90625e-05, 'epoch': 4.63}
{'loss': 1.2358, 'grad_norm': 1.0517865419387817, 'learning_rate': 1.953125e-05, 'epoch': 4.81}
{'loss': 1.2241, 'grad_norm': 1.5404359102249146, 'learning_rate': 0.0, 'epoch': 5.0}
{'eval_loss': 1.2270289659500122, 'eval_accuracy': 0.8565789473684211, 'eval_micro_f1': 0.8565789473684211, 'eval_macro_f1': 0.8524422981043359, 'eval_runtime': 11.0833, 'eval_samples_per_second': 68.571, 'eval_steps_per_second': 0.541, 'epoch': 5.0}
{'train_runtime': 155.5152, 'train_samples_per_second': 219.914, 'train_steps_per_second': 1.736, 'train_loss': 1.2807570492779767, 'epoch': 5.0}
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8566
  eval_loss               =      1.227
  eval_macro_f1           =     0.8524
  eval_micro_f1           =     0.8566
  eval_runtime            = 0:00:10.83
  eval_samples_per_second =      70.17
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 216/270 [01:53<00:16,  3.35it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [02:19<00:00,  2.08it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [02:35<00:00,  1.74it/s]
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:10<00:00,  1.76s/it]
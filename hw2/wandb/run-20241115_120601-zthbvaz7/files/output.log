[2024-11-15 12:06:04,625] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/15/2024 12:06:05 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmp_toa0x8e/test.c -o /tmp/tmp_toa0x8e/test.o
11/15/2024 12:06:05 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmp_toa0x8e/test.o -laio -o /tmp/tmp_toa0x8e/a.out
11/15/2024 12:06:05 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmp2ckkzl1h/test.c -o /tmp/tmp2ckkzl1h/test.o
11/15/2024 12:06:05 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmp2ckkzl1h/test.o -L/data/align-anything/miniconda3/envs/hantao_cham -L/data/align-anything/miniconda3/envs/hantao_cham/lib64 -lcufile -o /tmp/tmp2ckkzl1h/a.out
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                                                                                                                                                                     | 0/270 [00:00<?, ?it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                   | 54/270 [00:38<01:21,  2.66it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.1935, 'grad_norm': 6.551551342010498, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.19}
{'loss': 0.9642, 'grad_norm': 2.237305164337158, 'learning_rate': 1.953125e-05, 'epoch': 0.37}
{'loss': 0.8823, 'grad_norm': 3.5704596042633057, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.56}
{'loss': 0.8417, 'grad_norm': 3.1973259449005127, 'learning_rate': 1.7968750000000003e-05, 'epoch': 0.74}
{'loss': 0.7697, 'grad_norm': 8.714773178100586, 'learning_rate': 1.71875e-05, 'epoch': 0.93}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.641898512840271, 'eval_accuracy': 0.7526785714285714, 'eval_micro_f1': 0.7526785714285714, 'eval_macro_f1': 0.516627394564741, 'eval_runtime': 8.352, 'eval_samples_per_second': 134.099, 'eval_steps_per_second': 2.155, 'epoch': 1.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                         | 108/270 [01:16<01:12,  2.22it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.6827, 'grad_norm': 2.778820514678955, 'learning_rate': 1.6406250000000002e-05, 'epoch': 1.11}
{'loss': 0.653, 'grad_norm': 4.1715545654296875, 'learning_rate': 1.5625e-05, 'epoch': 1.3}
{'loss': 0.6432, 'grad_norm': 7.494429588317871, 'learning_rate': 1.4843750000000002e-05, 'epoch': 1.48}
{'loss': 0.6371, 'grad_norm': 7.24978494644165, 'learning_rate': 1.4062500000000001e-05, 'epoch': 1.67}
{'loss': 0.5968, 'grad_norm': 5.028146266937256, 'learning_rate': 1.3281250000000001e-05, 'epoch': 1.85}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.5548347234725952, 'eval_accuracy': 0.7866071428571428, 'eval_micro_f1': 0.7866071428571428, 'eval_macro_f1': 0.5974965318501598, 'eval_runtime': 11.0804, 'eval_samples_per_second': 101.08, 'eval_steps_per_second': 1.624, 'epoch': 2.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 162/270 [01:54<00:43,  2.48it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.625, 'grad_norm': 3.9326744079589844, 'learning_rate': 1.25e-05, 'epoch': 2.04}
{'loss': 0.5468, 'grad_norm': 4.889098644256592, 'learning_rate': 1.171875e-05, 'epoch': 2.22}
{'loss': 0.5379, 'grad_norm': 4.266054153442383, 'learning_rate': 1.0937500000000002e-05, 'epoch': 2.41}
{'loss': 0.5461, 'grad_norm': 5.675222873687744, 'learning_rate': 1.0156250000000001e-05, 'epoch': 2.59}
{'loss': 0.478, 'grad_norm': 3.383660078048706, 'learning_rate': 9.375000000000001e-06, 'epoch': 2.78}
{'loss': 0.5377, 'grad_norm': 4.17557430267334, 'learning_rate': 8.59375e-06, 'epoch': 2.96}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.516652524471283, 'eval_accuracy': 0.7910714285714285, 'eval_micro_f1': 0.7910714285714285, 'eval_macro_f1': 0.6468931975433607, 'eval_runtime': 10.4015, 'eval_samples_per_second': 107.677, 'eval_steps_per_second': 1.731, 'epoch': 3.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 216/270 [02:31<00:22,  2.39it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.5231, 'grad_norm': 3.6197168827056885, 'learning_rate': 7.8125e-06, 'epoch': 3.15}
{'loss': 0.4352, 'grad_norm': 3.8882384300231934, 'learning_rate': 7.031250000000001e-06, 'epoch': 3.33}
{'loss': 0.4821, 'grad_norm': 3.717357873916626, 'learning_rate': 6.25e-06, 'epoch': 3.52}
{'loss': 0.4745, 'grad_norm': 5.2151618003845215, 'learning_rate': 5.468750000000001e-06, 'epoch': 3.7}
{'loss': 0.4593, 'grad_norm': 4.959453105926514, 'learning_rate': 4.6875000000000004e-06, 'epoch': 3.89}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.5244354009628296, 'eval_accuracy': 0.7866071428571428, 'eval_micro_f1': 0.7866071428571428, 'eval_macro_f1': 0.6308653283398891, 'eval_runtime': 9.5213, 'eval_samples_per_second': 117.631, 'eval_steps_per_second': 1.89, 'epoch': 4.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [02:59<00:00,  2.26it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.4249, 'grad_norm': 3.8411624431610107, 'learning_rate': 3.90625e-06, 'epoch': 4.07}
{'loss': 0.4148, 'grad_norm': 2.6840267181396484, 'learning_rate': 3.125e-06, 'epoch': 4.26}
{'loss': 0.4066, 'grad_norm': 3.953737497329712, 'learning_rate': 2.3437500000000002e-06, 'epoch': 4.44}
{'loss': 0.4029, 'grad_norm': 3.6521003246307373, 'learning_rate': 1.5625e-06, 'epoch': 4.63}
{'loss': 0.4264, 'grad_norm': 4.508052349090576, 'learning_rate': 7.8125e-07, 'epoch': 4.81}
{'loss': 0.451, 'grad_norm': 3.5877246856689453, 'learning_rate': 0.0, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [03:18<00:00,  1.36it/s]
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.                                                                                  
{'eval_loss': 0.5104475021362305, 'eval_accuracy': 0.7928571428571428, 'eval_micro_f1': 0.7928571428571428, 'eval_macro_f1': 0.6459222062868825, 'eval_runtime': 10.8963, 'eval_samples_per_second': 102.787, 'eval_steps_per_second': 1.652, 'epoch': 5.0}
{'train_runtime': 198.9523, 'train_samples_per_second': 86.754, 'train_steps_per_second': 1.357, 'train_loss': 0.593950026123612, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:07<00:00,  2.28it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.7929
  eval_loss               =     0.5104
  eval_macro_f1           =     0.6459
  eval_micro_f1           =     0.7929
  eval_runtime            = 0:00:08.00
  eval_samples_per_second =    139.936
  eval_steps_per_second   =      2.249

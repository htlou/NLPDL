[2024-11-15 11:34:36,289] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/15/2024 11:34:36 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmplnkvh1b5/test.c -o /tmp/tmplnkvh1b5/test.o
11/15/2024 11:34:36 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmplnkvh1b5/test.o -laio -o /tmp/tmplnkvh1b5/a.out
11/15/2024 11:34:37 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmp58i8avtq/test.c -o /tmp/tmp58i8avtq/test.o
11/15/2024 11:34:37 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmp58i8avtq/test.o -L/data/align-anything/miniconda3/envs/hantao_cham -L/data/align-anything/miniconda3/envs/hantao_cham/lib64 -lcufile -o /tmp/tmp58i8avtq/a.out
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                                                                                                                                                                     | 0/270 [00:00<?, ?it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                   | 54/270 [00:43<01:48,  2.00it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.0068, 'grad_norm': 2.5089468955993652, 'learning_rate': 1.925925925925926e-05, 'epoch': 0.19}
{'loss': 0.8957, 'grad_norm': 3.340021848678589, 'learning_rate': 1.851851851851852e-05, 'epoch': 0.37}
{'loss': 0.8016, 'grad_norm': 3.060425281524658, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.56}
{'loss': 0.7134, 'grad_norm': 9.30260944366455, 'learning_rate': 1.7037037037037038e-05, 'epoch': 0.74}
{'loss': 0.7044, 'grad_norm': 11.09630012512207, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.93}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.5422971844673157, 'eval_accuracy': 0.775, 'eval_micro_f1': 0.775, 'eval_macro_f1': 0.5575071106277514, 'eval_runtime': 9.5299, 'eval_samples_per_second': 117.525, 'eval_steps_per_second': 1.889, 'epoch': 1.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                         | 108/270 [01:23<01:21,  1.99it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.6132, 'grad_norm': 10.11657428741455, 'learning_rate': 1.555555555555556e-05, 'epoch': 1.11}
{'loss': 0.5895, 'grad_norm': 9.659473419189453, 'learning_rate': 1.4814814814814815e-05, 'epoch': 1.3}
{'loss': 0.5774, 'grad_norm': 26.758628845214844, 'learning_rate': 1.4074074074074075e-05, 'epoch': 1.48}
{'loss': 0.5572, 'grad_norm': 13.541359901428223, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.67}
{'loss': 0.5088, 'grad_norm': 8.817797660827637, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.85}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.4246046841144562, 'eval_accuracy': 0.8267857142857142, 'eval_micro_f1': 0.8267857142857142, 'eval_macro_f1': 0.7155959336936654, 'eval_runtime': 9.2603, 'eval_samples_per_second': 120.946, 'eval_steps_per_second': 1.944, 'epoch': 2.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 162/270 [02:02<00:46,  2.31it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.5406, 'grad_norm': 9.614907264709473, 'learning_rate': 1.1851851851851852e-05, 'epoch': 2.04}
{'loss': 0.4329, 'grad_norm': 7.257429599761963, 'learning_rate': 1.1111111111111113e-05, 'epoch': 2.22}
{'loss': 0.4151, 'grad_norm': 11.195120811462402, 'learning_rate': 1.037037037037037e-05, 'epoch': 2.41}
{'loss': 0.4137, 'grad_norm': 9.640145301818848, 'learning_rate': 9.62962962962963e-06, 'epoch': 2.59}
{'loss': 0.3744, 'grad_norm': 10.99339485168457, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.78}
{'loss': 0.435, 'grad_norm': 9.915257453918457, 'learning_rate': 8.148148148148148e-06, 'epoch': 2.96}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.38369762897491455, 'eval_accuracy': 0.8526785714285714, 'eval_micro_f1': 0.8526785714285714, 'eval_macro_f1': 0.7602770757615479, 'eval_runtime': 8.9037, 'eval_samples_per_second': 125.79, 'eval_steps_per_second': 2.022, 'epoch': 3.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 216/270 [02:42<00:26,  2.06it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.4031, 'grad_norm': 12.50289249420166, 'learning_rate': 7.4074074074074075e-06, 'epoch': 3.15}
{'loss': 0.3292, 'grad_norm': 15.727725982666016, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.33}
{'loss': 0.3401, 'grad_norm': 12.104878425598145, 'learning_rate': 5.925925925925926e-06, 'epoch': 3.52}
{'loss': 0.3368, 'grad_norm': 14.41736125946045, 'learning_rate': 5.185185185185185e-06, 'epoch': 3.7}
{'loss': 0.2852, 'grad_norm': 6.582630634307861, 'learning_rate': 4.444444444444444e-06, 'epoch': 3.89}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.41522836685180664, 'eval_accuracy': 0.8491071428571428, 'eval_micro_f1': 0.8491071428571428, 'eval_macro_f1': 0.750422497321699, 'eval_runtime': 9.2865, 'eval_samples_per_second': 120.605, 'eval_steps_per_second': 1.938, 'epoch': 4.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [03:12<00:00,  2.35it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.2956, 'grad_norm': 7.639487266540527, 'learning_rate': 3.7037037037037037e-06, 'epoch': 4.07}
{'loss': 0.3403, 'grad_norm': 8.352452278137207, 'learning_rate': 2.962962962962963e-06, 'epoch': 4.26}
{'loss': 0.264, 'grad_norm': 6.9416913986206055, 'learning_rate': 2.222222222222222e-06, 'epoch': 4.44}
{'loss': 0.2507, 'grad_norm': 9.485825538635254, 'learning_rate': 1.4814814814814815e-06, 'epoch': 4.63}
{'loss': 0.2472, 'grad_norm': 7.694194316864014, 'learning_rate': 7.407407407407407e-07, 'epoch': 4.81}
{'loss': 0.322, 'grad_norm': 15.195244789123535, 'learning_rate': 0.0, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [03:32<00:00,  1.27it/s]
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.                                                                                  
{'eval_loss': 0.44706490635871887, 'eval_accuracy': 0.8473214285714286, 'eval_micro_f1': 0.8473214285714286, 'eval_macro_f1': 0.7424435189795994, 'eval_runtime': 10.0131, 'eval_samples_per_second': 111.853, 'eval_steps_per_second': 1.798, 'epoch': 5.0}
{'train_runtime': 212.9944, 'train_samples_per_second': 81.035, 'train_steps_per_second': 1.268, 'train_loss': 0.48125134309132894, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:09<00:00,  1.95it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.8527
  eval_loss               =     0.3837
  eval_macro_f1           =     0.7603
  eval_micro_f1           =     0.8527
  eval_runtime            = 0:00:09.50
  eval_samples_per_second =    117.886
  eval_steps_per_second   =      1.895

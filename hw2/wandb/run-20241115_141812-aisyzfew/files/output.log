[2024-11-15 14:18:14,930] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/15/2024 14:18:15 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmpgf2zmuqx/test.c -o /tmp/tmpgf2zmuqx/test.o
11/15/2024 14:18:15 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmpgf2zmuqx/test.o -laio -o /tmp/tmpgf2zmuqx/a.out
11/15/2024 14:18:15 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -fPIC -O2 -isystem /data/align-anything/miniconda3/envs/hantao_cham/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /data/align-anything/miniconda3/envs/jy-s/include -I/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/include -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs -c /tmp/tmpkyugvnj6/test.c -o /tmp/tmpkyugvnj6/test.o
11/15/2024 14:18:15 - INFO - root - /data/align-anything/miniconda3/envs/jy-s/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/data/align-anything/miniconda3/envs/jy-s/lib -Wl,-rpath-link,/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib -L/data/align-anything/miniconda3/envs/jy-s/targets/x86_64-linux/lib/stubs /tmp/tmpkyugvnj6/test.o -L/data/align-anything/miniconda3/envs/hantao_cham -L/data/align-anything/miniconda3/envs/hantao_cham/lib64 -lcufile -o /tmp/tmpkyugvnj6/a.out
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                                                                                                                                                                     | 0/535 [00:00<?, ?it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                  | 107/535 [00:50<02:09,  3.30it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 1.3918, 'grad_norm': 4.268624305725098, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.09}
{'loss': 1.3021, 'grad_norm': 3.78195858001709, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.19}
{'loss': 1.097, 'grad_norm': 4.354527950286865, 'learning_rate': 1.988188976377953e-05, 'epoch': 0.28}
{'loss': 0.814, 'grad_norm': 4.272055149078369, 'learning_rate': 1.9488188976377956e-05, 'epoch': 0.37}
{'loss': 0.5998, 'grad_norm': 3.030226469039917, 'learning_rate': 1.909448818897638e-05, 'epoch': 0.47}
{'loss': 0.472, 'grad_norm': 3.6759674549102783, 'learning_rate': 1.8700787401574803e-05, 'epoch': 0.56}
{'loss': 0.3816, 'grad_norm': 8.734188079833984, 'learning_rate': 1.830708661417323e-05, 'epoch': 0.65}
{'loss': 0.3951, 'grad_norm': 4.902752876281738, 'learning_rate': 1.7913385826771654e-05, 'epoch': 0.75}
{'loss': 0.3649, 'grad_norm': 4.773845672607422, 'learning_rate': 1.751968503937008e-05, 'epoch': 0.84}
{'loss': 0.3139, 'grad_norm': 3.064359426498413, 'learning_rate': 1.7125984251968505e-05, 'epoch': 0.93}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.25551822781562805, 'eval_accuracy': 0.9236842105263158, 'eval_micro_f1': 0.9236842105263158, 'eval_macro_f1': 0.9219804169574968, 'eval_runtime': 8.1517, 'eval_samples_per_second': 93.232, 'eval_steps_per_second': 1.472, 'epoch': 1.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                         | 214/535 [01:48<02:27,  2.17it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.3201, 'grad_norm': 2.262092351913452, 'learning_rate': 1.673228346456693e-05, 'epoch': 1.03}
{'loss': 0.2621, 'grad_norm': 4.672577381134033, 'learning_rate': 1.6338582677165356e-05, 'epoch': 1.12}
{'loss': 0.227, 'grad_norm': 1.9210807085037231, 'learning_rate': 1.5944881889763783e-05, 'epoch': 1.21}
{'loss': 0.3301, 'grad_norm': 2.7630228996276855, 'learning_rate': 1.5551181102362206e-05, 'epoch': 1.31}
{'loss': 0.2412, 'grad_norm': 2.922792911529541, 'learning_rate': 1.5157480314960632e-05, 'epoch': 1.4}
{'loss': 0.2586, 'grad_norm': 3.4110281467437744, 'learning_rate': 1.4763779527559057e-05, 'epoch': 1.5}
{'loss': 0.2203, 'grad_norm': 4.476715087890625, 'learning_rate': 1.437007874015748e-05, 'epoch': 1.59}
{'loss': 0.2597, 'grad_norm': 3.7392771244049072, 'learning_rate': 1.3976377952755906e-05, 'epoch': 1.68}
{'loss': 0.2648, 'grad_norm': 2.6800806522369385, 'learning_rate': 1.3582677165354331e-05, 'epoch': 1.78}
{'loss': 0.2669, 'grad_norm': 5.005642414093018, 'learning_rate': 1.3188976377952758e-05, 'epoch': 1.87}
{'loss': 0.2297, 'grad_norm': 3.8934898376464844, 'learning_rate': 1.2795275590551182e-05, 'epoch': 1.96}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.21042637526988983, 'eval_accuracy': 0.9276315789473685, 'eval_micro_f1': 0.9276315789473685, 'eval_macro_f1': 0.9258927618802106, 'eval_runtime': 11.4184, 'eval_samples_per_second': 66.559, 'eval_steps_per_second': 1.051, 'epoch': 2.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 321/535 [02:52<01:32,  2.32it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.1927, 'grad_norm': 2.2375972270965576, 'learning_rate': 1.2401574803149607e-05, 'epoch': 2.06}
{'loss': 0.1534, 'grad_norm': 2.4678146839141846, 'learning_rate': 1.2007874015748033e-05, 'epoch': 2.15}
{'loss': 0.1744, 'grad_norm': 4.579717636108398, 'learning_rate': 1.1614173228346456e-05, 'epoch': 2.24}
{'loss': 0.1891, 'grad_norm': 3.208153486251831, 'learning_rate': 1.1220472440944883e-05, 'epoch': 2.34}
{'loss': 0.1964, 'grad_norm': 4.1160359382629395, 'learning_rate': 1.0826771653543309e-05, 'epoch': 2.43}
{'loss': 0.1616, 'grad_norm': 4.611299514770508, 'learning_rate': 1.0433070866141732e-05, 'epoch': 2.52}
{'loss': 0.1456, 'grad_norm': 4.345186710357666, 'learning_rate': 1.0039370078740158e-05, 'epoch': 2.62}
{'loss': 0.1871, 'grad_norm': 4.264523983001709, 'learning_rate': 9.645669291338583e-06, 'epoch': 2.71}
{'loss': 0.1386, 'grad_norm': 3.8244378566741943, 'learning_rate': 9.251968503937008e-06, 'epoch': 2.8}
{'loss': 0.1511, 'grad_norm': 2.971092700958252, 'learning_rate': 8.858267716535434e-06, 'epoch': 2.9}
{'loss': 0.203, 'grad_norm': 3.567049741744995, 'learning_rate': 8.46456692913386e-06, 'epoch': 2.99}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.20087315142154694, 'eval_accuracy': 0.9421052631578948, 'eval_micro_f1': 0.9421052631578948, 'eval_macro_f1': 0.9407824949192636, 'eval_runtime': 12.5504, 'eval_samples_per_second': 60.556, 'eval_steps_per_second': 0.956, 'epoch': 3.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 428/535 [03:53<00:48,  2.22it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.127, 'grad_norm': 2.3005504608154297, 'learning_rate': 8.070866141732285e-06, 'epoch': 3.08}
{'loss': 0.1151, 'grad_norm': 2.5399885177612305, 'learning_rate': 7.677165354330708e-06, 'epoch': 3.18}
{'loss': 0.0883, 'grad_norm': 1.0986576080322266, 'learning_rate': 7.283464566929135e-06, 'epoch': 3.27}
{'loss': 0.1634, 'grad_norm': 3.2991340160369873, 'learning_rate': 6.88976377952756e-06, 'epoch': 3.36}
{'loss': 0.1027, 'grad_norm': 3.546473979949951, 'learning_rate': 6.496062992125984e-06, 'epoch': 3.46}
{'loss': 0.1276, 'grad_norm': 4.75512170791626, 'learning_rate': 6.1023622047244104e-06, 'epoch': 3.55}
{'loss': 0.1095, 'grad_norm': 1.9257166385650635, 'learning_rate': 5.708661417322835e-06, 'epoch': 3.64}
{'loss': 0.1207, 'grad_norm': 1.8199920654296875, 'learning_rate': 5.314960629921261e-06, 'epoch': 3.74}
{'loss': 0.1381, 'grad_norm': 3.9940176010131836, 'learning_rate': 4.921259842519686e-06, 'epoch': 3.83}
{'loss': 0.1066, 'grad_norm': 4.200672626495361, 'learning_rate': 4.52755905511811e-06, 'epoch': 3.93}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):                                                                                                                                                                                                                                  
{'eval_loss': 0.2090049684047699, 'eval_accuracy': 0.9381578947368421, 'eval_micro_f1': 0.9381578947368421, 'eval_macro_f1': 0.936798300150762, 'eval_runtime': 9.793, 'eval_samples_per_second': 77.607, 'eval_steps_per_second': 1.225, 'epoch': 4.0}
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 535/535 [04:46<00:00,  2.15it/s]/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
{'loss': 0.1445, 'grad_norm': 3.552785873413086, 'learning_rate': 4.1338582677165355e-06, 'epoch': 4.02}
{'loss': 0.0795, 'grad_norm': 3.0266754627227783, 'learning_rate': 3.740157480314961e-06, 'epoch': 4.11}
{'loss': 0.0813, 'grad_norm': 3.325432300567627, 'learning_rate': 3.346456692913386e-06, 'epoch': 4.21}
{'loss': 0.0798, 'grad_norm': 2.9208502769470215, 'learning_rate': 2.952755905511811e-06, 'epoch': 4.3}
{'loss': 0.0793, 'grad_norm': 8.48303508758545, 'learning_rate': 2.5590551181102365e-06, 'epoch': 4.39}
{'loss': 0.0999, 'grad_norm': 1.6800960302352905, 'learning_rate': 2.165354330708662e-06, 'epoch': 4.49}
{'loss': 0.0752, 'grad_norm': 1.781052589416504, 'learning_rate': 1.7716535433070868e-06, 'epoch': 4.58}
{'loss': 0.0699, 'grad_norm': 3.8411920070648193, 'learning_rate': 1.377952755905512e-06, 'epoch': 4.67}
{'loss': 0.1193, 'grad_norm': 4.115981101989746, 'learning_rate': 9.84251968503937e-07, 'epoch': 4.77}
{'loss': 0.0898, 'grad_norm': 4.929029941558838, 'learning_rate': 5.905511811023623e-07, 'epoch': 4.86}
{'loss': 0.0946, 'grad_norm': 2.322636127471924, 'learning_rate': 1.968503937007874e-07, 'epoch': 4.95}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 535/535 [05:03<00:00,  1.76it/s]
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.                                                                                  
{'eval_loss': 0.22536304593086243, 'eval_accuracy': 0.9355263157894737, 'eval_micro_f1': 0.9355263157894737, 'eval_macro_f1': 0.933987346250293, 'eval_runtime': 8.5709, 'eval_samples_per_second': 88.672, 'eval_steps_per_second': 1.4, 'epoch': 5.0}
{'train_runtime': 303.8206, 'train_samples_per_second': 112.566, 'train_steps_per_second': 1.761, 'train_loss': 0.2649759058640382, 'epoch': 5.0}
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
/data/align-anything/miniconda3/envs/hantao_cham/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]
***** eval metrics *****
  epoch                   =        5.0
  eval_accuracy           =     0.9421
  eval_loss               =     0.2009
  eval_macro_f1           =     0.9408
  eval_micro_f1           =     0.9421
  eval_runtime            = 0:00:09.31
  eval_samples_per_second =     81.583
  eval_steps_per_second   =      1.288
